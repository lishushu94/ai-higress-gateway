# 指标写库优化建议

当前指标在请求路径中同步写入 `provider_routing_metrics_history`，为避免热点分钟内频繁落库，可考虑以下方案：

## 写入减载
- **本地/Redis 缓冲 + 定时冲刷**：在应用进程内维护按分钟分桶的累积计数（成功/失败/QPS/延迟求和），通过后台定时任务（如 Celery Beat/worker）每 30~60 秒批量写库并重置内存桶，失败时重试，避免请求线程阻塞。
- **批量 UPSERT**：冲刷时使用批量 `INSERT ... ON CONFLICT DO UPDATE` 或数据库提供的批量更新接口，一次写入多条 `(provider_id, logical_model, transport, is_stream, user_id, api_key_id, bucket_minute)` 维度的聚合结果，减少事务和锁开销。
- **可配置采样**：在高并发时对成功请求启用按比例采样，失败请求全量记录；采样率配置通过环境变量或管理接口动态调整，确保可观测性与写库压力的平衡。
- **异步队列**：在写库前先写入消息队列/Redis Stream，专门的消费进程按批聚合后写库，避免核心路径阻塞，并便于水平扩展。

### 已实现（v0）
- 网关进程内置指标缓冲器：默认每 30 秒落盘一次（`METRICS_FLUSH_INTERVAL_SECONDS`），并在桶数过大时异步触发刷新。
- 采样：成功请求可按 `METRICS_SUCCESS_SAMPLE_RATE` 采样，失败请求必定记录。
- 统计字段：按桶聚合总数/成功/失败、平均延迟 + 样本分位（P95/P99 以样本加权近似）、错误率、成功 QPS，并基于错误率推导 `healthy/degraded/unhealthy`。

## 统计精度与字段扩展
- **延迟分位数**：在内存桶中维护近似直方图（如 TDigest/CKMS），冲刷时计算 p50/p90/p95/p99，减少平均值偏差。
- **错误分类**：新增 `error_type` 或 `error_code` 维度（如超时、上游 5xx、鉴权失败、限流等），便于定位故障主因，可按需聚合写入扩展表或 JSONB 字段。
- **上游信息**：记录 `provider_endpoint` 或路由策略版本号，排查不同上游的指标差异。
- **健康状态来源**：将 `healthy` 字段区分为 `healthy_by_metric`（基于错误率/延迟阈值评估）与 `healthy_by_probe`（主动探活结果），支持后续切流策略使用。
- **请求大小/模型 token**：可选记录 `request_tokens`、`response_tokens` 或 `payload_bytes`，为成本和性能分析提供基础。

## 系统配合
- **补写离线聚合**：保留分钟级原始表，但通过定时作业（SQL/OLAP/Celery 周期任务）汇总到 5 分钟/1 小时表，供看板读取，进一步降低查询压力。
- **保护机制**：对写库失败设定熔断/降级逻辑（如只保留内存计数、暂存队列、限速写入），防止指标路径反向拖慢主流程。

这些措施可组合使用：先在应用内做轻量内存聚合和批量 UPSERT，若写压仍高，再引入异步队列和多层汇总表，以达到写入可控、指标足够精确的平衡。
