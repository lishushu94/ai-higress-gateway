# Redis configuration
# For docker-compose: redis://redis:6379/0
# For local development: redis://localhost:6379/0
REDIS_PASSWORD=change-me-redis  # pragma: allowlist secret
REDIS_URL=redis://:${REDIS_PASSWORD}@172.20.0.10:16379/0  # pragma: allowlist secret

# PostgreSQL 配置
# docker-compose: postgresql+psycopg://apiproxy:change-me-postgres@172.20.0.12:15432/apiproxy  # pragma: allowlist secret
# local: postgresql+psycopg://apiproxy:change-me-postgres@localhost:15432/apiproxy  # pragma: allowlist secret
POSTGRES_USER=apiproxy
POSTGRES_PASSWORD=change-me-postgres  # pragma: allowlist secret
POSTGRES_DB=apiproxy
POSTGRES_HOST=172.20.0.12
DATABASE_URL=postgresql+psycopg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:15432/${POSTGRES_DB}  # pragma: allowlist secret

# API authentication token for gateway clients (plain text, will be base64-encoded in headers)
APIPROXY_AUTH_TOKEN=timeline

# Secret key for encrypting sensitive fields (e.g.,存储到 Redis 前的加密密钥)
# 可使用 scripts/generate_secret_key.sh 快速生成
SECRET_KEY=please-change-me

# Logging configuration
LOG_LEVEL=INFO
# Uncomment to force a specific timezone for log timestamps (e.g., Asia/Shanghai)
# LOG_TIMEZONE=Asia/Shanghai

############################
# Multi-provider routing  ##
############################

# Comma-separated provider ids used by the multi-provider routing layer.
# 注意：下面的示例使用了"yourprovider"作为占位符，请替换为您自己的提供商ID
# Example: openai,azure,yourprovider
LLM_PROVIDERS=openai,azure,yourprovider

# OpenAI provider configuration
LLM_PROVIDER_openai_NAME=OpenAI
LLM_PROVIDER_openai_BASE_URL=https://api.openai.com
LLM_PROVIDER_openai_API_KEY=sk-your-openai-api-key
# 可选：多 Key 轮询（逗号分隔简写）
# LLM_PROVIDER_openai_API_KEYS=sk-key-a,sk-key-b
# 可选：带权重/限速的多 Key JSON
# LLM_PROVIDER_openai_API_KEYS_JSON=[{"key":"sk-key-a","weight":2},{"key":"sk-key-b","max_qps":5}]
LLM_PROVIDER_openai_MODELS_PATH=/v1/models
LLM_PROVIDER_openai_WEIGHT=3
LLM_PROVIDER_openai_REGION=global
LLM_PROVIDER_openai_COST_INPUT=0.003
LLM_PROVIDER_openai_COST_OUTPUT=0.006
LLM_PROVIDER_openai_MAX_QPS=50

# Gemini (HTTP 模式) provider configuration
LLM_PROVIDER_gemini_NAME=Gemini
LLM_PROVIDER_gemini_BASE_URL=https://generativelanguage.googleapis.com
LLM_PROVIDER_gemini_API_KEY=your-gemini-api-key
# 若使用新版 v1beta 建议覆盖 models 路径
LLM_PROVIDER_gemini_MODELS_PATH=/v1beta/models

# Azure OpenAI provider configuration
LLM_PROVIDER_azure_NAME=Azure OpenAI
LLM_PROVIDER_azure_BASE_URL=https://your-resource.openai.azure.com
LLM_PROVIDER_azure_API_KEY=your-azure-api-key
LLM_PROVIDER_azure_MODELS_PATH=/openai/models
LLM_PROVIDER_azure_WEIGHT=2
LLM_PROVIDER_azure_REGION=us-east
LLM_PROVIDER_azure_COST_INPUT=0.0025
LLM_PROVIDER_azure_COST_OUTPUT=0.005
LLM_PROVIDER_azure_MAX_QPS=100

# Custom provider configuration (optional)
# 注意：将"yourprovider"替换为您在LLM_PROVIDERS中定义的实际提供商ID
LLM_PROVIDER_yourprovider_NAME=Custom Provider
LLM_PROVIDER_yourprovider_BASE_URL=http://localhost:8080
LLM_PROVIDER_yourprovider_API_KEY=not-required
LLM_PROVIDER_yourprovider_MODELS_PATH=/v1/models
# 留空表示该提供商仅支持 Chat Completions，需要自动回退到 /v1/chat/completions
LLM_PROVIDER_yourprovider_MESSAGES_PATH=/v1/message
# 注意：权重要大于0（Pydantic中gt=0），否则配置会被跳过。如果想弱化某个厂商，
# 可以将权重调整为较小的正数（例如0.1），而不是设成0。
LLM_PROVIDER_yourprovider_WEIGHT=1
LLM_PROVIDER_yourprovider_REGION=local
LLM_PROVIDER_yourprovider_COST_INPUT=0.001
LLM_PROVIDER_yourprovider_COST_OUTPUT=0.003
LLM_PROVIDER_yourprovider_MAX_QPS=20

# Google Gemini 官方 SDK 直连示例（TRANSPORT=sdk 时不会拼接 /v1/...）
# 注意：如果不希望走 SDK，请保持默认 TRANSPORT=http。
LLM_PROVIDER_google_NAME=Google Native SDK
LLM_PROVIDER_google_BASE_URL=https://generativelanguage.googleapis.com
LLM_PROVIDER_google_TRANSPORT=sdk
LLM_PROVIDER_google_API_KEY=your-gemini-api-key
# 可选：如果不想调用 SDK 的 list()，可用静态模型列表
# LLM_PROVIDER_google_STATIC_MODELS_JSON=["gemini-2.5-flash","gemini-1.5-pro"]

# Claude/Anthropic 官方 SDK 直连示例（TRANSPORT=sdk 时不会拼接 /v1/...）
LLM_PROVIDER_claude_NAME=Claude
LLM_PROVIDER_claude_BASE_URL=https://api.anthropic.com
LLM_PROVIDER_claude_TRANSPORT=sdk
LLM_PROVIDER_claude_API_KEY=your-claude-api-key

# 静态模型列表配置（适用于没有/models接口的中转服务）
# 当某些中转服务或自定义服务没有提供 /models 接口时，可以使用静态配置声明模型列表
# 注意：将 "local" 替换为您在 LLM_PROVIDERS 中定义的实际提供商ID

# 方式一：直接在配置文件中定义模型列表（二选一）
# LLM_PROVIDER_<实际提供商ID>_STATIC_MODELS_JSON=["model-a", {"id":"model-b","context_length":16384}]
# 示例（假设您在LLM_PROVIDERS中定义了"myprovider"）：
# LLM_PROVIDER_myprovider_STATIC_MODELS_JSON=["gpt-3.5-turbo", "gpt-4", {"id":"custom-model","context_length":4096}]

# 方式二：从外部JSON文件加载模型列表（二选一）
# LLM_PROVIDER_<实际提供商ID>_STATIC_MODELS_FILE=./configs/<实际提供商ID>-models.json
# 示例（假设您在LLM_PROVIDERS中定义了"myprovider"）：
# LLM_PROVIDER_myprovider_STATIC_MODELS_FILE=./configs/myprovider-models.json

# 取消注释并修改下面的示例以适应您的配置：
# 注意：将"yourprovider"替换为您在LLM_PROVIDERS中定义的实际提供商ID
# LLM_PROVIDER_yourprovider_STATIC_MODELS_JSON=["model-a", {"id":"model-b","context_length":16384}]
# LLM_PROVIDER_yourprovider_STATIC_MODELS_FILE=./configs/yourprovider-models.json
