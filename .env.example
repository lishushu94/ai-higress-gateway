# Redis configuration
# For docker-compose: redis://redis:6379/0
# For local development: redis://localhost:6379/0
REDIS_PASSWORD=timeline-redis-2025
REDIS_URL=redis://:timeline-redis-2025@172.20.0.10:6379/0

# API authentication token for gateway clients (plain text, will be base64-encoded in headers)
APIPROXY_AUTH_TOKEN=timeline

# Logging configuration
LOG_LEVEL=INFO
# Uncomment to force a specific timezone for log timestamps (e.g., Asia/Shanghai)
# LOG_TIMEZONE=Asia/Shanghai

############################
# Multi-provider routing  ##
############################

# Comma-separated provider ids used by the multi-provider routing layer.
# 注意：下面的示例使用了"yourprovider"作为占位符，请替换为您自己的提供商ID
# Example: openai,azure,yourprovider
LLM_PROVIDERS=openai,azure,yourprovider

# OpenAI provider configuration
LLM_PROVIDER_openai_NAME=OpenAI
LLM_PROVIDER_openai_BASE_URL=https://api.openai.com
LLM_PROVIDER_openai_API_KEY=sk-your-openai-api-key
LLM_PROVIDER_openai_MODELS_PATH=/v1/models
LLM_PROVIDER_openai_WEIGHT=3
LLM_PROVIDER_openai_REGION=global
LLM_PROVIDER_openai_COST_INPUT=0.003
LLM_PROVIDER_openai_COST_OUTPUT=0.006
LLM_PROVIDER_openai_MAX_QPS=50

# Azure OpenAI provider configuration
LLM_PROVIDER_azure_NAME=Azure OpenAI
LLM_PROVIDER_azure_BASE_URL=https://your-resource.openai.azure.com
LLM_PROVIDER_azure_API_KEY=your-azure-api-key
LLM_PROVIDER_azure_MODELS_PATH=/openai/models
LLM_PROVIDER_azure_WEIGHT=2
LLM_PROVIDER_azure_REGION=us-east
LLM_PROVIDER_azure_COST_INPUT=0.0025
LLM_PROVIDER_azure_COST_OUTPUT=0.005
LLM_PROVIDER_azure_MAX_QPS=100

# Custom provider configuration (optional)
# 注意：将"yourprovider"替换为您在LLM_PROVIDERS中定义的实际提供商ID
LLM_PROVIDER_yourprovider_NAME=Custom Provider
LLM_PROVIDER_yourprovider_BASE_URL=http://localhost:8080
LLM_PROVIDER_yourprovider_API_KEY=not-required
LLM_PROVIDER_yourprovider_MODELS_PATH=/v1/models
# 注意：权重要大于0（Pydantic中gt=0），否则配置会被跳过。如果想弱化某个厂商，
# 可以将权重调整为较小的正数（例如0.1），而不是设成0。
LLM_PROVIDER_yourprovider_WEIGHT=1
LLM_PROVIDER_yourprovider_REGION=local
LLM_PROVIDER_yourprovider_COST_INPUT=0.001
LLM_PROVIDER_yourprovider_COST_OUTPUT=0.003
LLM_PROVIDER_yourprovider_MAX_QPS=20

# 静态模型列表配置（适用于没有/models接口的中转服务）
# 当某些中转服务或自定义服务没有提供 /models 接口时，可以使用静态配置声明模型列表
# 注意：将 "local" 替换为您在 LLM_PROVIDERS 中定义的实际提供商ID

# 方式一：直接在配置文件中定义模型列表（二选一）
# LLM_PROVIDER_<实际提供商ID>_STATIC_MODELS_JSON=["model-a", {"id":"model-b","context_length":16384}]
# 示例（假设您在LLM_PROVIDERS中定义了"myprovider"）：
# LLM_PROVIDER_myprovider_STATIC_MODELS_JSON=["gpt-3.5-turbo", "gpt-4", {"id":"custom-model","context_length":4096}]

# 方式二：从外部JSON文件加载模型列表（二选一）
# LLM_PROVIDER_<实际提供商ID>_STATIC_MODELS_FILE=./configs/<实际提供商ID>-models.json
# 示例（假设您在LLM_PROVIDERS中定义了"myprovider"）：
# LLM_PROVIDER_myprovider_STATIC_MODELS_FILE=./configs/myprovider-models.json

# 取消注释并修改下面的示例以适应您的配置：
# 注意：将"yourprovider"替换为您在LLM_PROVIDERS中定义的实际提供商ID
# LLM_PROVIDER_yourprovider_STATIC_MODELS_JSON=["model-a", {"id":"model-b","context_length":16384}]
# LLM_PROVIDER_yourprovider_STATIC_MODELS_FILE=./configs/yourprovider-models.json
